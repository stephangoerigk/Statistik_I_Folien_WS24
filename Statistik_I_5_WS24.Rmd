---
title: "Einheit 5"
subtitle: "âš”<br/>with xaringan"
author: "Prof. Dr. Stephan Goerigk"
institute: "RStudio, PBC"
date: "2016/12/12 (updated: `r Sys.Date()`)"
output:
  xaringan::moon_reader:
    css: [xaringan-themer.css, "hygge"]
    lib_dir: libs
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
    seal: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)

library(tidyverse)
library(kableExtra)
library(ggplot2)
library(plotly)
library(htmlwidgets)
library(MASS)
library(ggpubr)
library(xaringanthemer)
library(xaringanExtra)

style_duo_accent(
  primary_color = "#621C37",
  secondary_color = "#EE0071",
  background_image = "blank.png"
)

xaringanExtra::use_xaringan_extra(c("tile_view"))

use_scribble(
  pen_color = "#EE0071",
  pen_size = 4
)

knitr::opts_chunk$set(
  fig.retina = TRUE,
  warning = FALSE,
  message = FALSE
)
```

name: Title slide
class: middle, left
<br><br><br><br><br><br><br>
# Statistik I
***
### Einheit 5: Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen
##### `r format(as.Date(data.frame(readxl::read_excel("Modul Quantitative Methoden I_Termine.xlsx"))$Datum), "%d.%m.%Y")[5]` | Prof. Dr. Stephan Goerigk

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen 

#### Wiederholung:

**Inferenzstatistik: **

* Umfasst alle statistischen Verfahren, die es erlauben, trotz der InformationsunvollstÃ¤ndigkeit der Stichprobendaten Aussagen Ã¼ber eine Population zu treffen.

**Population: **

* Gesamtheit aller MerkmalstrÃ¤ger:innen, auf die eine Untersuchungsfrage gerichtet ist.

**Stichprobe: **

* Auswahl bestimmter MerkmalstrÃ¤ger:innen aus einer Population

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen 

#### Wiederholung:

<small>

**Problem:**

* Wenn nur ein Teil der Grundgesamtheit erfasst wird, z.B. 100 Personen, ist die **Informationslage** in Bezug auf die Untersuchungsfrage **unvollstÃ¤ndig**. Wir kÃ¶nnen nicht einfach deskriptiv-statistische Methoden verwenden.

* Wie kann man trotzdem Aussagen treffen, die sich auf alle Personen der Grundgesamtheit beziehen, obwohl nur die Daten einer Stichprobe vorliegen?

***

**Idee:**

* Wir ziehen die Personen zufÃ¤llig aus der Population in die Stichprobe.

* Wir greifen auf mathematische Methoden zur Formalisierung von Zufallsprozessen zurÃ¼ck $\rightarrow$ Wahrscheinlichkeitstheorie

* Aus diesen ergeben sich Methoden, die RÃ¼ckschlÃ¼sse von der Stichprobe auf die Population erlauben $\rightarrow$ Inferenzstatistik

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen 

#### Logik des SchlieÃŸens von Stichprobe auf Population (Einzelschritte folgen)

.center[
```{r eval = TRUE, echo = F, out.width = "450px"}
knitr::include_graphics("bilder/Population.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen 

#### Logik des SchlieÃŸens von Stichprobe auf Population (Einzelschritte folgen)

.center[
```{r eval = TRUE, echo = F, out.width = "900px"}
knitr::include_graphics("bilder/zfg.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

<small>

**Inferenzstatistik:**

* Schluss von Zufallsstichprobe auf Population

* Grundlage: Wahrscheinlichkeitsrechnung

* Zentral: Zufallsprozesse (Ausgang unsicher, nicht mit Sicherheit vorhersagbar)

***

**Wahrscheinlichkeitsrechnung:**

.center[*Mathematik ist der Versuch, alles zu bÃ¤ndigen, auch den Zufall.*

Rudolf Taschner]

* Statistischer Wahrscheinlichkeitsbegriff geht zurÃ¼ck auf 17. Jahrhundert (Frankreich)

* Im Jahr 1654 wandte sich der GlÃ¼cksspieler Chevalier de Mere mit mehreren Fragen an den franzÃ¶sischen Mathematiker Blaise Pascal

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Stochastik:**

* Stochastik = die Kunst des Vermutens (altgriechisch)

* Mathematik setzt Vorstellung von Zufall voraus (= Modelle von Situationen, deren Ausgang unsicher ist)

* Keine Einzelereignisse vorhersagbar, aber:

* Erkennen von RegelmÃ¤ÃŸigkeiten bei VorgÃ¤ngen, deren Ergebnisse vom Zufall abhÃ¤ngen.

* Zentraler Begriff: Zufallsexperiment

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment:**

Im Prinzip beliebig oft wiederholbarer Vorgang, der nach bestimmter Vorschrift ausgefÃ¼hrt wird, wobei das Ergebnis vom Zufall abhÃ¤ngt, d.h. der Ausgang kann nicht eindeutig im voraus bestimmt werden.

* Folge von gleichartigen, voneinander unabhÃ¤ngigen Versuchen mÃ¶glich.

* Entweder Folge voneinander unabhÃ¤ngiger Versuche mit einem Objekt oder jeweils einmaliger Versuche mit â€gleichartigenâ€ (unabhÃ¤ngigen) Objekten.

Beispiele:

1. Ein WÃ¼rfel wird wiederholte Male geworfen und es wird beobachtet, wie oft jede Zahl kommt.

2. ParteiprÃ¤ferenz bei weiblichen Jugendlichen zwischen 16 und 18 Jahren.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment - Nomenklatur:**

* Die mÃ¶glichen Ergebnisse eines Zufallsexperimentes heiÃŸen Elementarereignisse Ï‰

* Die Menge aller mÃ¶glichen Ergebnisse eines Zufallsexperimentes bezeichnet man als Ereignisraum Î©.

* Beispiel: â€™Einmaliges WÃ¼rfelnâ€™: Elementarereignisse sind {1}, {2}, {3}, {4}, {5}, {6}. Ereignisraum $Î© = {1, 2, 3, 4, 5, 6}$.

* Ereignis A: Teilmenge des Ereignisraums, z.B. alle geraden Augenzahlen beim WÃ¼rfeln. Es gilt: $Ï‰ âˆˆ A, A âŠ‚ Î©$

* Sicheres Ereignis: Jenes Ereignis, welches unter gegebenen Bedingungen immer eintritt.

* UnmÃ¶gliches Ereignis: Jenes Ereignis, welches unter gegebenen Bedingungen nie eintritt.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Definition der statistischen Wahrscheinlichkeit:**

<small>

Die Wahrscheinlichkeit fÃ¼r das Auftreten eines Ereignisses A, $P_{(A)}$, ist jener Wert, bei dem sich die relative HÃ¤ufigkeit $r_{n}(A)$ bei
$n$ $\rightarrow$ âˆ Versuchen unter gleichen Bedingungen stabilisiert.

Die mathematische Formulierung:

.center[
```{r eval = TRUE, echo = F, out.width = "250px"}
knitr::include_graphics("bilder/stat_Wahrscheinlichkeit.png")
```
]

***

In anderen Worten:

* Die Wahrscheinlichkeit eines Ereignisses gibt an, mit welcher relativen HÃ¤ufigkeit das Ereignis eintrÃ¤te, wenn man den Versuch theoretisch unendlich oft wiederholen wÃ¼rde.

* Sie sagt jedoch nichts darÃ¼ber aus, wie hÃ¤ufig das Ereignis bei einer kleinen Anzahl von Versuchen, z.B. $n = 5$, auftritt.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Laplace-Wahrscheinlichkeit**


Bei Zufallsexperimenten, bei denen nur endlich viele, gleichwahrscheinliche Ergebnisse mÃ¶glich sind, ergibt sich fÃ¼r ein beliebiges Ereignis A die Wahrscheinlichkeit $P(A):$

.center[
```{r eval = TRUE, echo = F, out.width = "750px"}
knitr::include_graphics("bilder/Laplace.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Axiome der Wahrscheinlichkeitsrechnung nach Kolmogoroff**

Wahrscheinlichkeiten lassen sich durch drei Eigenschaften, die auch fÌˆÃ¼r relative HÃ¤ufigkeiten gelten, und aus denen sich alle Rechenregeln fÃ¼r Wahrscheinlichkeiten ableiten lassen, charakterisieren:


1. FÃ¼r die Wahrscheinlichkeit eines Ereignisses **gilt stets**: 
$$0 â‰¤ P_{(A)} â‰¤ 1$$

2. Die Wahrscheinlichkeit eines **sicheren Ereignisses** betrÃ¤gt 
$$P_{(Î©)} = 1$$

3. **Additionsregel der Wahrscheinlichkeit:** Die Wahrscheinlichkeit, dass eines von $k$ einander ausschlieÃŸenden Ereignissen auftritt, ist die Summe der einzelnen Wahrscheinlichkeiten $P_{(A_{1})}, P_{(A_{2})}, . . . , P_{(A_{k})}$.

$$P_{(A_{1} âˆ¨ A_{2} âˆ¨...âˆ¨ A_{k})}=P_{(A_{1})} + P_{(A_{2})} +...+ P_{(A_{k})}$$
---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Rechenregeln: UnmÃ¶gliches Ereignis**

* Die Wahrscheinlichkeit des unmÃ¶glichen Ereignisses B betrÃ¤gt 
$$P_{(B)} = 0$$
 
* Wenn B ein unmÃ¶gliches Ereignis ist, kann es nie eintreten: 
$$rn_{(B)} = 0 â†’ P_{(B)} = 0$$

ACHTUNG: 

* Aus $P_{(B)} = 0$ folgt nicht, dass B ein unmÃ¶gliches Ereignis ist. 

* Das bedeutet nur, dass der Grenzwert der relativen HÃ¤ufigkeit bei $n â†’ âˆ$ Null ist, woraus aber nicht folgt, dass B nie eintreten kann! (Analoges gilt fÃ¼r $P_{(A)} = 1$).

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Rechenregeln: KomplementÃ¤rereignis**

* $P_{(A)}+P_{(\bar{A})}=1,P_{(\bar{A})}=1âˆ’_{P(A)}$

* $\bar{A}$ tritt immer dann ein, wenn $A$ nicht eintritt â†’ $r_{n}(A) + r_{n}(\bar{A}) = 1$

***

Beispiel - MÃ¼nzwurf: 

$P_{(K)} + P_{(Z)} = 0.5 + 0.5 = 1$


---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Stochastische UnabhÃ¤ngigkeit von Ereignissen**

* Beim Ziehen mit ZurÃ¼cklegen sind die einzelnen Wahrscheinlichkeiten gleich und die Ziehungen stochastisch unabhÃ¤ngig.

* Beim Ziehen ohne ZurÃ¼cklegen Ã¤ndern sich mit jeder Ziehung die Anteile der â€™gÃ¼nstigenâ€™ $Ï‰_i$ , und daher auch die Wahrscheinlichkeiten. Die Ziehungen sind daher stochastisch abhÃ¤ngig.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment**

* Jedes mÃ¶gliche Ergebnis aus einem Zufallsexperiment nennen wir ein Elementarereignis $Ï‰$

* Die Menge aller mÃ¶glichen Ereignisse ist definiert als der Ereignisraum $Î©$

* Der Ereignisraum $Î©$ heiÃŸt diskret, wenn er aus abzÃ¤hlbar vielen Elementarereignissen besteht

* Der Ereignisraum $Î©$ heiÃŸt stetig, wenn er aus Ã¼berabzÃ¤hlbar vielen Elementarereignissen besteht

* Zufallsexperiment ist ein allgemeiner Begriff, der Grundlage fÃ¼r die Inferenzstatistik ist

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsexperiment**

* Zufallsexperiment 1: 

  Einmaliger Wurf mit einer MÃ¼nze; $Î© =$ { Kopf, Zahl } endlich und abzÃ¤hlbar

* Zufallsexperiment 2: 

   Wurf mit einem WÃ¼rfel so lange bis zum ersten Mal drei â€™Einserâ€™ hintereinander kommen; wir interessieren uns fÃ¼r die Anzahl der notwendigen WÃ¼rfe; $Î© =$ 3,4,5,Â·Â·Â· oder $Î© =$ {k : k natÃ¼rliche Zahl â‰¥ 3}; $Î©$ ist diskret und abzÃ¤hlbar unendlich

* Zufallsexperiment 3: 

  Lebensdauer einer GlÃ¼hbirne; $Î© =$ ${x : x â‰¥ 0}$; $Î©$ ist stetig und Ã¼berabzÃ¤hlbar unendlich

* Richtige Bestimmung von $Î©$ ist Voraussetzung fÃ¼r Richtigkeit jeder weiteren statistischen Analyse

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Psychologische Fragestellungen:

* Praktisch alle psychologischen Theorien enthalten Aussagen Ã¼ber Populationen (nicht nur Ã¼ber isolierte Stichproben).

* Zu ihrer empirischen ÃœberprÃ¼fung sind dann **immer** inferenzstatistische Methoden notwendig.

***

Beispiele fÃ¼r psychologische Fragestellungen:

* **Beispiel 1 (diskret):**
  * Wir interessieren uns fÃ¼r die relative HÃ¤ufigkeit $h_{A}$ der Personen in Europa, die an AngststÃ¶rungen erkrankt sind.

* **Beispiel 2 (stetig):**
  * Wir interessieren uns fÃ¼r den Mittelwert $\bar{x}_{IQ}$ und die empirische Varianz $s^2_{empIQ}$ des
Intelligenzquotienten (IQ) von Personen in Europa.


---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsvariable**

* Es seien ein Wahrscheinlichkeitsraum $Î©$ und $p_{(Ï‰)}$ fÃ¼r alle $Ï‰$ gegeben.

* Eine mathematische Funktion $X$, welche jedem Ereignis Ï‰ eine reelle Zahl $X_{(Ï‰)}$ zuweist, heiÃŸt Zufallsvariable (ZV).

* $X$ ist eine Zufallsvariable (ZV), wenn die Werte von $X$ reelle Zahlen sind, die durch ein Zufallsexperiment bestimmt werden, und wenn fÃ¼r die Ereignisse, die man damit beschreiben kann, Wahrscheinlichkeiten angebbar sind.

* Der Wert, den die ZV bei der DurchfÃ¼hrung des Zufallsexperimentes annimmt, heiÃŸt Realisation von $X$, und wird mit $x$ bezeichnet.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Zufallsvariable**

* Zufallsvariable lÃ¤sst sich durch ihre Wahrscheinlichkeitsfunktion beschreiben, welche angibt, mit welcher Wahrscheinlichkeit die einzelnen Realisationen $x_{i}$ auftreten.

* Es sei $p_{i}$ die Wahrscheinlichkeit des Auftretens des Wertes $x_{i}$; dann ist

$$f(x_{i})=P(X=x_{i})=p_{i}; p_{i} âˆˆ[0,1]$$

* Wenn alle mÃ¶glichen AusprÃ¤gungen von X berÃ¼cksichtigt wurden,ist die Summe aller mÃ¶glichen Einzelwahrscheinlichkeiten $p_{i}$ = 1

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### ZufÃ¤llige Ziehung einer einzelnen Person

ZufÃ¤llige Ziehung einer **einzelnen** Person aus einer Population von $N$ Personen:

Dieser Vorgang ist ein **Zufallsexperiment**:

* Wir wissen im Voraus nicht, welche Person gezogen wird.

* Die Ergebnismenge $Î©$ ist die Menge aller Personen in der Population:

$$Î© = {ğ‘ƒğ‘’ğ‘Ÿğ‘ ğ‘œğ‘›_{1},ğ‘ƒğ‘’ğ‘Ÿğ‘ ğ‘œğ‘›_{ 2},...,ğ‘ƒğ‘’ğ‘Ÿğ‘ ğ‘œğ‘›_{ğ‘–},...,ğ‘ƒğ‘’ğ‘Ÿğ‘ ğ‘œn _{ğ‘}}$$
* Wir setzen voraus, dass jede Person i in der Population die **gleiche Wahrscheinlichkeit** hat, gezogen zu werden.

* Alle Elementarereignisse haben die gleiche Wahrscheinlichkeit:

$$P({Person_{i}})=\frac{1}{N} $$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Beispiel AngststÃ¶rungen:

<small>

* Wir interessieren uns fÃ¼r die relative HÃ¤ufigkeit $h_{A}$ der Personen in Deutschland, die an AngststÃ¶rungen erkrankt sind.

* Sei $N_{A}$ die Anzahl der Angstpatienten in der Population und $A_{A}$ die Menge der Angstpatienten in der Population:

$$A_{A} = {Patient_{1},Patient_{ 2},...,Patient_{ğ‘–},...,Patient_{ğ‘}}$$

* Die relative HÃ¤ufigkeit der Angstpatienten in der Population ist also $h_{A}=\frac{N_{A}}{N}$

* Die Wahrscheinlichkeit, zufÃ¤llig eine Angstpatient:in zu ziehen ist:

$$P(A_{A}) = P({Patient_{1}}) + P({Patient_{2}}) + P({Patient_{N_{A}}})$$
* Die Wahrscheinlichkeit dafÃ¼r, zufÃ¤llig eine Angstpatient:in zu ziehen, entspricht also der relativen HÃ¤ufigkeit der AngststÃ¶rung in der Population:

$$P(A_{A}) = h_{A}$$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Beispiel AngststÃ¶rungen:

* Sei nun $X$ eine Zufallsvariable, die den Wert 1 annimmt, falls die zufÃ¤llig gezogene Person eine AngststÃ¶rung hat, und 0, falls nicht.

* Diese Zufallsvariable ist eine Bernoulli-Variable und folgt somit einer Bernoulli-Verteilung.

* Der Parameter $ğœ‹$ der Bernoulli-Verteilung entspricht der Wahrscheinlichkeit, dass $X$ den Wert 1 annimmt, also der Wahrscheinlichkeit, eine Angstpatient:in zu ziehen. 

* Diese Wahrscheinlichkeit entspricht wiederum der relativen HÃ¤ufigkeit der AngststÃ¶rung in der Population (siehe letzte Folie).

Formal:

$$ğœ‹= P(X=1)= P(A_{A}) = h_{A}$$


---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Beispiel AngststÃ¶rungen:

Zusammengefasst: Unter der Voraussetzung, dass 
* jede Person in der Population die gleiche Wahrscheinlichkeit hat, gezogen zu werden,
* $X$ eine Zufallsvariable ist, die den Wert 1 annimmt, falls die gezogene Person eine AngststÃ¶rung hat, und 0, falls nicht,

folgt X einer Bernoulli-Verteilung und der Wert des Parameters $ğ…$ dieser Bernoulli-Verteilung ist identisch mit dem Wert der relativen HÃ¤ufigkeit $h_{A}$ der AngststÃ¶rung in der Population.

* Wenn wir herausfinden wollen, wie hoch die relative HÃ¤ufigkeit der AngststÃ¶rung in der Population ist, mÃ¼ssen wir lediglich herausfinden, welchen Wert der Parameter $ğœ‹$ hat.
* Wenn wir z.B. wÃ¼ssten, dass $ğœ‹ = 0.3$ ist, wÃ¼ssten wir auch, dass die relative HÃ¤ufigkeit der AngststÃ¶rung in der Population $h_{A}$ = 0.3 ist.
* Da $ğœ‹$ ein Parameter einer Wahrscheinlichkeitsverteilung ist, kÃ¶nnen wir das Problem der Bestimmung einer deskriptivstatistischen MaÃŸzahl in der Population $(h_{A})$ komplett in die Wahrscheinlichkeitstheorie verlagern und somit alle Mittel verwenden, die uns diese zur VerfÃ¼gung stellt.


---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Dichtefunktion**

* Eine stetige ZV $X$ kann jeden Wert in einem Intervall [a, b] annehmen

* Die Wahrscheinlichkeiten der einzelnen AusprÃ¤gungen (Werte) einer stetigen ZV kÃ¶nnen (im Gegensatz zum diskreten Fall) nicht angegeben werden

* Es kÃ¶nnen nur Wahrscheinlichkeiten $f(x)dx$ angegeben werden, mit welchen die Werte innerhalb von Intervallen $dx$ um die Werte $x$ auftreten

* Beispielsweise fragt man nicht, wie viele Personen exakt 1.75 Meter groÃŸ sind, sondern z.B., wie viele Personen zwischen 1.75 und 1.76 Meter groÃŸ sind

* Die Funktion $f(x)$ heiÃŸt Dichtefunktion

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Dichtefunktion**

* Die Wahrscheinlichkeit, dass die ZV Werte zwischen a und b annimmt, wird dann allgemein definiert als das Integral Ã¼ber die Dichtefunktion mit Integrationsgrenzen a und b.

* Analog zum diskreten Fall erhÃ¤lt man durch Integration die Verteilungsfunktion

* Die Wahrscheinlichkeit ist definiert als FlÃ¤che unter der Dichtefunktion

.center[
```{r eval = TRUE, echo = F, out.width = "350px"}
knitr::include_graphics("bilder/dichte1.png")
```
]

Es gilt fuÌˆr alle $a<b$:

.center[
```{r eval = TRUE, echo = F, out.width = "550px"}
knitr::include_graphics("bilder/dichte2.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Erwartungswert**

Beispiel: $X$ ist die erhaltene Augenzahl bei einmaligem WÃ¼rfeln; die Wahrscheinlichkeitsverteilung von $X$ ist:

.center[
```{r eval = TRUE, echo = F, out.width = "350px"}
knitr::include_graphics("bilder/erwartungswert1.png")
```
]
* Welchen Wert â€™erwartenâ€™ wir, wenn wir dieses Zufallsexperiment sehr lange durchfÃ¼hren?

* Intuitiv erwarten wir $X = 1$ bei $\frac{1}{6}$ der WÃ¼rfe, $X = 2$ bei  $\frac{1}{6}$ bei der WÃ¼rfe, usw.

* Der Durchschnitt von $X$ auf lange Sicht ist der Erwartungswert von $X$

* Der Erwartungswert einer ZV ist ein MaÃŸ fÃ¼r das Zentrum der Verteilung

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Varianz der ZV**

Die Varianz $Ïƒ^2$ ist ein StreuungsmaÃŸ der Verteilung

$$Ïƒ_{X}^2 =E[(Xâˆ’E[X])^2]=E[X^2]âˆ’(E[X])^2$$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

**Varianz der ZV**

Beispiel: $X$ ist die beobachtete Augenzahl bei einmaligem WÃ¼rfeln; die Wahrscheinlichkeitsverteilung von $X$ ist

.center[
```{r eval = TRUE, echo = F, out.width = "650px"}
knitr::include_graphics("bilder/varianz.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

<small>

**Î±-Quantil**

Als Î±-Quantil $q_{Î±}$ wird ein Wert bezeichnet, unterhalb dessen ein vorgegebener Anteil $Î±$ aller FÃ¤lle der Verteilung liegen

* Jeder Wert unterhalb von $q_{Î±}$ unterschreitet den Anteil $Î±$, mit $Î±$ als reelle Zahl zwischen 0 (gar kein Fall der Verteilung) und 1 (alle FÃ¤lle oder 100% der Verteilung)

* FÃ¼r stetige ZV gilt:

.center[
```{r eval = TRUE, echo = F, out.width = "450px"}
knitr::include_graphics("bilder/alpha_quantil.png")
```
]

* FÃ¼r diskrete ZV gilt (Aufrunden zur nÃ¤chsten ganzzahligen AusprÃ¤gung):

.center[
```{r eval = TRUE, echo = F, out.width = "450px"}
knitr::include_graphics("bilder/alpha_quantil2.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

<small>

* Diese Verteilung beschreibt eine ZV, welche die Zahlen $1,2,Â·Â·Â· ,m$ annehmen kann, und es gilt:

.center[
```{r eval = TRUE, echo = F, out.width = "450px"}
knitr::include_graphics("bilder/gleichverteilung1.png")
```
]

* Anwendung bei Zufallsexperimenten, deren Ergebnisse gleich hÃ¤ufig sind, also wenn angenommen wird, dass die $m$ Elementarereignisse gleichwahrscheinlich sind

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

Beispiel: 

X = die erhaltene Augenzahl bei einmaligem WÃ¼rfeln

.center[
```{r eval = TRUE, echo = F, out.width = "250px"}
knitr::include_graphics("bilder/gleichverteilung3.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Diskrete Gleichverteilung**

.center[
```{r echo = F, out.width="350px", out.height="350px"}
require(ggplot2)
require(grid)

x1  <- 3:17
df <- data.frame(x = x1, y = rdunif(1, 1, 1))

plot1 <- ggplot(df, aes(x = x, y = y)) + geom_bar(stat = "identity", col = "black", fill = "black") + 
  scale_y_continuous(expand = c(0.01, 0)) +
  coord_cartesian(ylim = c(0,2)) +
  xlab("1 bis m") + ylab("1/m") + 
  labs(title = "Gleichverteilung") + 
  theme_classic() +
  theme(plot.title = element_text(size = rel(1.2), vjust = 1.5), text = element_text(size = 25), axis.text.y = element_blank())

print(plot1)
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

* Wir betrachten ein Zufallsexperiment mit 2 AusgÃ¤ngen, â€™Erfolg (2)â€™ und â€™Misserfolg (1)â€™

* Die Wahrscheinlichkeit fÃ¼r Erfolg sei $p$, mit $p$ zwischen 0 und 1

* Wir fÃ¼hren dieses Experiment n-mal durch, wobei zwischen den einzelnen DurchfÃ¼hrungen UnabhÃ¤ngigkeit angenommen wird (â€™Ziehen mit ZurÃ¼cklegenâ€™)

* Die ZV $X$ beschreibt die Anzahl der Erfolge und ist binomialverteilt mit Parametern $n$ und $p$, $X$ ~ $B(n, p)$

.center[
```{r eval = TRUE, echo = F, out.width = "550px"}
knitr::include_graphics("bilder/binomial1.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

.center[
```{r echo = F, out.width="350px", out.height="350px"}
require(ggplot2)
require(grid)

x1  <- 3:17
df <- data.frame(x = x1, y = dbinom(x1, 20, 0.5))

plot1 <- ggplot(df, aes(x = x, y = y)) + geom_bar(stat = "identity", col = "black", fill = "black") + 
  scale_y_continuous(expand = c(0.01, 0)) + xlab("x") + ylab("Dichte") + 
  labs(title = "P(X=2) = 0.5, n = 20") + 
  theme_classic() +
  theme(plot.title = element_text(size = rel(1.2), vjust = 1.5), text = element_text(size = 25))

print(plot1)
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

* Beispiel: Ein GlÃ¼cksrad besteht aus 20 Feldern, wobei 5 davon Gewinnfelder sind. 

* Wie groÃŸ ist die Wahrscheinlichkeit, dass Sie zwei Mal gewinnen, wenn Sie das GlÃ¼cksrad drei Mal drehen?

* Experiment mit 2 AusgÃ¤ngen, Erfolg (5 Gewinnfelder) und Misserfolg

* $n = 3$, weil wir das GlÃ¼cksrad drei Mal drehen

* $p = \frac{5}{20} = 0.25$ ist die Wahrscheinlichkeit zum Erfolg

.center[
```{r eval = TRUE, echo = F, out.width = "550px"}
knitr::include_graphics("bilder/binomial2.png")
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle diskrete Verteilungen

**Binomialverteilung**

<small>

* Binomialverteilte ZV nimmt Werte zwischen 0 und $n$ an

* Binomialverteilung ist symmetrisch fÃ¼r $p = 0.5$

* Je kleiner/grÃ¶ÃŸer $p$ desto rechts/links-schiefer die Verteilung

* Summe mehrerer Bernoulli-Variablen

Erwartungswert und Varianz:

$$E[X]=np$$ 

$$Ïƒ^2 =np(1âˆ’p)$$

* FÃ¼r $n = 1$: $B(1, p)$ ist eine Bernoulli-ZV mit Erwartungswert $p$ und Varianz $p(1 âˆ’ p)$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Normalverteilung (NV)**

<small>

* Die NV ist eine stetige Verteilung, die durch 2 Parameter $Î¼$ und $Ïƒ$ charakterisiert ist

* Es sei $X$ eine ZV die $N(Î¼,Ïƒ^2)$ verteilt ist; $X$ kann Werte zwischen $âˆ’âˆ$ und $+âˆ$ annehmen

Dichtefunktion $Ï†_{(x)}$:

.center[
```{r eval = TRUE, echo = F, out.width = "350px"}
knitr::include_graphics("bilder/nv1.png")
```
]

* Geht $x$ â†’ $Â±âˆ$ strebt $Ï†(x)$ gegen 0

* $Ï†(x)$ ist symmetrisch um $Î¼$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Normalverteilung (NV)**

* $Ïƒ$ gibt den Abstand zwischen $Î¼$ und den Wendepunkten der Dichtefunktion an

* Wendepunkte an den Stellen $Î¼Â±Ïƒ$

* Wenn $Ïƒ$ groÃŸ ist, ist die Verteilung breit und niedrig, wenn $Ïƒ$ klein ist, ist die Verteilung schmal und hoch

* FlÃ¤che unter $Ï†(x)$ zwischen $âˆ’âˆ$ und $+âˆ$ ist gleich 1

* Die FlÃ¤che $Î¼ Â± Ïƒ$ umfasst ca. 68% aller FÃ¤lle

* Die FlÃ¤che $Î¼ Â± 2Ïƒ$ umfasst ca. 95% aller FÃ¤lle

* Es existieren unendlich viele NV durch beliebige Auswahl von $Î¼$ und $Ïƒ$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Normalverteilung (NV)**

.center[
```{r echo=FALSE, out.width="350px", out.height="350px"}
x = rnorm(100, mean = 0, sd = 1)
ggplot(data = data.frame(x = c(-6 * sd(x) + mean(x), 6 * sd(x) + mean(x))), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = mean(x), sd = sd(x)+2)) + 
  ylab("Ï†(x)") +
  xlab("") +
  ggtitle("N(1,3)") +
  scale_x_continuous(breaks = c(mean(x) - (sd(x)+2), mean(x), mean(x) + (sd(x)+2)), labels = c("-Ïƒ","Î¼", "+Ïƒ")) +
  scale_y_continuous(breaks = NULL) +
  geom_vline(xintercept = mean(x), linetype = "dashed", colour = "red") +
  geom_vline(xintercept = sd(x) + 2, linetype = "dashed", colour = "red") +
    geom_vline(xintercept = mean(x) - (sd(x)+2), linetype = "dashed", colour = "red") +
  theme_classic() +
  theme(text = element_text(size = 25))
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Beispiel IQ:

<small>

.pull-left[
* Wir interessieren uns fÃ¼r den Mittelwert $\bar{x}_{IQ}$ und die empirische Varianz $s^2_{empIQ}$ des IQs von Personen in Europa

* Wir setzen voraus, dass das Histogramm der Variable IQ in der Population der Personen in Europa durch die Wahrscheinlichkeitsdichtefunktion einer Normalverteilung approximiert werden kann, d.h. dass das Histogramm die â€Formâ€œ der Dichte einer Normalverteilung hat.

* Dies ist eine **Annahme**, von der wir nicht wissen, ob sie zutrifft. Wir werden jedoch Methoden kennenlernen, um die PlausibilitÃ¤t dieser Annahme zu Ã¼berprÃ¼fen.

]

.pull-right[
.center[
```{r echo=FALSE, out.width="450px", out.height="450px"}

df <- data.frame(PF = rnorm(100000, mean = 100, sd = 15))
ggplot(df, aes(x = PF)) + 
    geom_histogram(aes(y =..density..),
                   breaks = seq(50, 150, by = 2), 
                   colour = "black", 
                   fill = "white", bins = 100) +
  scale_x_continuous( breaks = seq(50, 150, by = 50)) +
  stat_function(fun = dnorm, args = list(mean = mean(df$PF), sd = sd(df$PF))) +
  labs(x = "IQ", y = "relative HÃ¤ufigkeit") +
   theme_classic() +
  theme(text = element_text(size = 25))
```
]
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Beispiel IQ:

* AuÃŸerdem setzen wir wieder voraus, dass alle Personen die **gleiche Wahrscheinlichkeit** haben, gezogen zu werden.

* Sei nun $X$ eine Zufallsvariable, die fÃ¼r den IQ der zufÃ¤llig gezogenen Person steht.
* Man kann dann beweisen, dass diese Zufallsvariable $X$ einer Normalverteilung folgt und der Parameter $ğœ‡$ dieser Normalverteilung dem Mittelwert des IQs in der Population entspricht:
  
$$ğœ‡ = \bar{x}_{IQ}$$

* der Parameter $ğœ^2$ dieser Normalverteilung der empirischen Varianz des IQs in der Population entspricht:

$$ğœ^2 = s^2_{empIQ}$$

* Der Beweis hierfÃ¼r funktioniert Ã¤hnlich wie bei der Bernoulli-Verteilung, ist aber deutlich aufwendiger.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Beispiel IQ:

**Zusammengefasst:** Unter der Voraussetzung, dass
* das Histogramm des IQs in der Population der Personen in Deutschland durch die Wahrscheinlichkeitsdichtefunktion einer Normalverteilung approximiert werden kann,

* jede Person in der Population die gleiche Wahrscheinlichkeit hat, gezogen zu werden,

* $X$ eine Zufallsvariable ist, die fÃ¼r den IQ der gezogenen Person steht

folgt $X$ einer Normalverteilung und
  * der Wert des Parameters $ğ$ dieser Normalverteilung ist identisch mit dem Mittelwert $\bar{x}_{IQ}$ des IQs in der Population,
  * der Wert des Parameters $ğˆ^2$ dieser Normalverteilung ist identisch mit der empirischen Varianz $s^2_{empIQ}$ des IQs in der Population.


---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Standardnormalverteilung $N(0,1)$**

* Spezielle NV fÃ¼r $Î¼ = 0$ und $Ïƒ = 1$ (GauÃŸâ€™sche Glockenkurve)

* Verteilung der $N(0,1)$ ist tabelliert

* FlÃ¤che zwischen $Î¼ = 0$ und einem beliebigen Wert z ist ablesbar 

* Quantile der NV; 1-FlÃ¤che rechts von einem Wert z, und links von âˆ’z 

* Ist X $N(Î¼,Ïƒ^2)$ verteilt dann fÃ¼hrt die Transformation $\frac{Xâˆ’Î¼}{Ïƒ}$ auf eine $N(0,1)$ Verteilung

* Vorteil, da Quantile in Tabellen ablesbar (es mÃ¼ssen nicht jedes mal Integrale fÃ¼r Dichtefunktion berechnet werden)

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Wahrscheinlichkeitsrechnung als Grundlage der Inferenzstatistik

##### Spezielle stetige Verteilungen

**Standardnormalverteilung $N(0,1)$**

.center[
```{r echo=FALSE, out.width="350px", out.height="350px"}
x = rnorm(100, mean = 0, sd = 1)
ggplot(data = data.frame(x = c(-3 * sd(x) + mean(x), 3 * sd(x) + mean(x))), aes(x)) +
  stat_function(fun = dnorm, n = 101, args = list(mean = mean(x), sd = sd(x))) + 
  ylab("Ï†(x)") +
  xlab("") +
  ggtitle("N(0,1)") +
  scale_x_continuous(breaks = c(mean(x) - (sd(x)), mean(x), mean(x) + (sd(x))), labels = c("-1","0", "1")) +
  scale_y_continuous(breaks = NULL) +
  geom_vline(xintercept = mean(x), linetype = "dashed", colour = "red") +
  geom_vline(xintercept = sd(x) , linetype = "dashed", colour = "red") +
    geom_vline(xintercept = mean(x) - (sd(x)), linetype = "dashed", colour = "red") +
  theme_classic() +
  theme(text = element_text(size = 25))
```
]

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Nutzen von Wahrscheinlichkeitsverteilungen zur Quantifizierung des Stichprobenfehlers:

**Z.B. Standardnormalverteilung (N~0,1):**

* Quantile der Standardnormalverteilung sind tabelliert

Z-Tabelle

* Wahrscheinlichkeit fÃ¼r jeden z-Wert kann abgelesen werden

* Zusammensetzen des z-Werts aus Zeile (bis 1 Stelle nach dem Komma) und Spalte (2. Stelle nach dem Komma)

* Anhand der Tabelle kann abgelesen werden, wie wahrscheinlich die Werte einer Verteilung sind (angenommen die Variable ist normalverteilt)

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen


```{r echo = F}
z0 <- seq(0, 3, 0.1)
z00 <- seq(0, 0.09, 0.01)
m <- outer(z0, z00, FUN = function(z0, z00) pnorm(z0 + z00))
m <- cbind(z0, m)
colnames(m) <- c("z", format(z00, decimal.mark = ","))
m[,1] = round(m[,1], 2)
m[,2:ncol(m)] = round(m[,2:ncol(m)], 4)
m = as.data.frame(m)
m[,1] = as.character(m[,1])

m[1:20,] %>%
  kbl() %>%
  kable_classic(full_width = T, position = "left", font_size = 10)
```

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

Bedeutung der p-Werte

* Die Felder in der Tabelle geben Ihnen die Wahrscheinlichkeit $P$ an, dass genau der ausgewÃ¤hlte z-Wert oder ein kleinerer z-Wert auftritt.

* Die Wahrscheinlichkeit, die Sie in den Feldern der z Tabelle finden, entspricht der FlÃ¤che unter der Verteilung.

* Diese  FlÃ¤che ist das Integral der Dichtefunktion von $-âˆ$ bis z.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

**Beispiel: Orientierung in der z-Tabelle**

Aufgabe: Sie suchem den z-Wert 0.35.

**Schritt 1:** Schauen Sie die 1. Spalte an

In der ersten Spalte (senkrecht) finden Sie die ersten zwei Ziffern 0.3 des z-Werts. 

**Schritt 2:** Schauen Sie die 2. Spalte an

Die dritte Ziffer 0.05 (die zweite Nachkommastelle), findet sich in der 3. Spalte.

**Schritt 3:** Wahrscheinlichkeit finden

Das Feld, in dem sich nun die Zeile mit 0.3 und die Spalte mit 0.05 kreuzen, ist die gesuchte Wahrscheinlichkeit fÃ¼r 0.35 oder einen kleineren z-Wert also $P(X â‰¤ 0.35) =0.63683$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

Negative Werte in der z-Werte Tabelle:

Wie Sie wahrscheinlich gesehen haben, fÃ¤ngt die z-Tabelle bei 0 an. Was machen Sie also, wenn Ihr gegebener z-Wert negativ ist?

DafÃ¼r gibt es einen Trick: Die Standardnormalverteilung ist achsensymmetrisch (die Funktion spiegelt sich also an der y-Achse). Das heiÃŸt, sie verlÃ¤uft links und rechts von der y-Achse genau spiegelverkehrt.

Es gilt: $Î¦(âˆ’x) = 1âˆ’Î¦(x)$

Wenn Sie einen negativen z Wert haben, suchen Sie also zunÃ¤chst den dazugehÃ¶rigen positiven z-Wert. Dann rechnen Sie 1 minus den positiven z-Wert.

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

* Um die Standardnormalverteilung Tabelle nutzen zu kÃ¶nnen, brauchen Sie entweder einen gegebenen z-Wert oder eine gegebene Wahrscheinlichkeit.

* Die Berechnung eines z-Werts kann fÃ¼r jeden Wert einer normalverteilten Variable erfolgen

* Dieser Prozess nennt sich **z-Transformation** oder kurz **Standardisierung**

* DafÃ¼r braucht man nichts weiter als den Mittelwert und die Standardabweichung der Verteilung

$$z_{i} = \frac{x_{i} - \bar{x}}{s}$$

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

Vorteil der Standardisierung:

* Messwerte von Personen verschiedener Populationen sind oft nicht direkt **vergleichbar**, z.B. die Leistung eines MÃ¤dchens in KugelstoÃŸen mit jener eines Jungen

* Dennoch mÃ¶chte man ausdrÃ¼cken kÃ¶nnen, wie gut die beiden Leistungen innerhalb der Bezugsgruppe sind

* Der Standardmesswert $z_{i}$: bezieht den beobachteten Messwert $x_{i}$ der i-ten Person auf den Mittelwert $\bar{x}$ der Gruppe und drÃ¼ckt die Abweichung in Standardeinheiten $s$ aus

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

Beispiel: Standardisierung KugelstoÃŸen $(N=5)$; Vergleich Frauen (w) und MÃ¤nner (m)

.center[
```{r echo = F}
set.seed(123)
N = 5
df = data.frame(ID = paste0(rep(1:N)),
                 Meter_m = round(rnorm(N, 9, 2)),
                Meter_w = round(rnorm(N, 6, 2))
)
x = df$Meter_m
n = length(x)
df2 = df
df = as.data.frame(t(df))
#rownames(df) = NULL
kable(df[,], col.names = NULL)
```
]
.left[LÃ¶sungsweg (fÃ¼r 3. Mann):] 

$$\bar{x}=\frac{`r paste(x, collapse = " + ")`}{`r n`}=\frac{`r sum(x)`}{`r n`}=`r round(mean(x), 2)`$$
$$s=\sqrt{\frac{`r paste(paste0("(",x, "-", round(mean(x),2),")^2"), collapse = " + ")`}{`r n`-1}}=\sqrt{\frac{`r sum((x - mean(x))^2)`}{`r n-1`}}=`r round(sd(x), 2)`$$

$$z_{3}=\frac{`r x[3]` - `r round(mean(x), 2)`}{`r round(sd(x), 2)`}= `r round(scale(x)[3], 2)`$$
---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

**Standardnormalverteilung (N~0,1):**

Beispiel: Standardisierung KugelstoÃŸen $(N=5)$; Vergleich Frauen (w) und MÃ¤nner (m)

.center[
```{r echo = F}
set.seed(123)
N = 5
df = data.frame(ID = paste0(rep(1:N)),
                 Meter_m = round(rnorm(N, 9, 2)),
                Meter_w = round(rnorm(N, 6, 2))
)
x = df$Meter_m
n = length(x)
df2 = df
df = as.data.frame(t(df))
#rownames(df) = NULL
kable(df[,], col.names = NULL)
```

Nach der Standardisierung jeden Werts anhand Mittelwert und Standardabweichung der Referenzgruppe:

```{r echo = F}
set.seed(123)
N = 5
df = data.frame(ID = paste0(rep(1:N)),
                 z_m = round(rnorm(N, 9, 2)),
                z_w = round(rnorm(N, 6, 2))
)
df$z_m = round(scale(df$z_m), 2)
df$z_w = round(scale(df$z_w), 2)
x = df$z_m
n = length(x)
df2 = df
df = as.data.frame(t(df))
#rownames(df) = NULL
kable(df[,], col.names = NULL)
```
]

**Interpretation:** WÃ¤hrend z.B. die 2. Frau absolut weniger weit gestoÃŸen hat (7m) als der 2. Mann (9m), liegt sie relativ zum Mittel der Gruppen vor ihm (0.53 > -0.26).

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Nutzen von Wahrscheinlichkeitsverteilungen zur Quantifizierung des Stichprobenfehlers:

**Z.B. Standardnormalverteilung (N~0,1):**

Beispiel 1:

* gegeben sei eine normalverteilte Variable X mit Mittelwert von 11 und Varianz von 5.53

* Wie hoch ist die Wahrscheinlichkeit das X nicht mehr als 14.5 Punkte aufweist?

* ZunÃ¤chst berechnen wir den z-Wert fÃ¼r $X=14.5$ (siehe Standardisierung)

$$z=\frac{14.5-11}{\sqrt{5.53}}=1.49$$
* In der z-Tabelle schlagen wir nach, wie wahrscheinlich ein z-Wert von hÃ¶chstens 1.49 ist

$$P(Z \leq 1.49) = 0.9319$$
* Mit einer 93%-igen Wahrscheinlichkeit ist ein zufÃ¤llig aus der Verteilung gezogener Wert nicht grÃ¶ÃŸer als 14.5

---
class: top, left
### Stichprobe, Grundgesamtheit - Wahrscheinlichkeitstheorie und Verteilungen

#### Nutzen von Wahrscheinlichkeitsverteilungen zur Quantifizierung des Stichprobenfehlers:

**Z.B. Standardnormalverteilung (N~0,1):**

Beispiel 2:

* gegeben sei eine normalverteilte Variable X mit Mittelwert von 11 und Varianz von 5.53

* Wie hoch ist die Wahrscheinlichkeit, dass X mehr als 14.5 Punkte aufweist?

* ZunÃ¤chst berechnen wir den z-Wert fÃ¼r $X=14.5$ (siehe Standardisierung)

$$z=\frac{14.5-11}{\sqrt{5.53}}=1.49$$
* In der z-Tabelle schlagen wir nach, wie wahrscheinlich ein z-Wert von grÃ¶ÃŸer als 1.49 ist

$$P(Z > 1.49) = 1- P(Z \leq 1.49)= 1-0.9319=0.0681$$
* Mit einer 6.8%-igen Wahrscheinlichkeit ist ein zufÃ¤llig aus der Verteilung gezogener Wert grÃ¶ÃŸer als 14.5.


---
class: top, left
### Take-aways

.full-width[.content-box-gray[
* **Inferenzstatistik** ist ein wahrscheinlichkeitsbasierter Schluss von Zufallsstichprobe auf Population

* Variablen in der Population sind nicht vollstÃ¤ndig beobachtbar und daher **Zufallsvariablen** (diskret vs. stetig)

* **Wahrscheinlichkeitsfunktion** definiert welche Werte wir beim zufÃ¤lligen Ziehen mit welcher Wahrscheinlichkeit erwarten

* Der **Erwartungswert** ist das Zentrum der Verteilung und der wahrscheinlichste Wert 

* Unter der **Gleichverteilung** ist jedes Ereignis gleich wahrscheinlich

* **Binomialverteilung** lÃ¤sst uns Wahrscheinlichkeit fÃ¼r ein diskretes Ereignis mit 2 AusgÃ¤ngen berechnen

* **Normalverteilung** ist stetige Verteilung, die extremen Ereignissen geringere und durchschnittlichen Ereignissen hÃ¶here Wahrscheinlichkeit zuweist 
]
]

